{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8858707-96eb-4da5-96bf-b304efd39b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726124a2-bb59-4b22-a011-036defdb230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654087f8-d3f5-44f1-843b-d480479a2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8211335-3c9e-4d32-994a-415224a796d9",
   "metadata": {},
   "source": [
    "## Load Image into TF Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aa44f-c0da-472d-b27d-e515e7ae660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = os.path.abspath('')\n",
    "IMAGES_PATH = os.path.join(current_folder,'data','images')\n",
    "print(IMAGES_PATH)\n",
    "\n",
    "images = tf.data.Dataset.list_files(IMAGES_PATH + '/*.jpg', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f59f5ae-36ba-4d3a-8418-e197812d46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef984d-a63d-4822-ae59-793e1bcd96a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x): \n",
    "    byte_img = tf.io.read_file(x)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1070155-0bce-4ad9-bdcd-3879e3d53b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904a056-f13b-4188-b94e-1ff0fee83fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a8169-5ad3-45c1-8207-2ec51ba0e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = images.batch(4).as_numpy_iterator()\n",
    "plot_images = image_generator.next()\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image) \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38085862-f577-4e11-b23c-0b7a49487095",
   "metadata": {},
   "source": [
    "# Partition Unaugmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721c1ed-482c-41f5-b458-fbd09f55ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = os.path.abspath('')\n",
    "for folder in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join(current_folder,'data', folder, 'images')):\n",
    "        \n",
    "        filename = file.split('.')[0]+'.json'\n",
    "        existing_filepath = os.path.join(current_folder,'data','labels', filename)\n",
    "        if os.path.exists(existing_filepath): \n",
    "            new_filepath = os.path.join('data',folder,'labels',filename)\n",
    "            os.replace(existing_filepath, new_filepath)      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6627dcaf",
   "metadata": {},
   "source": [
    "## Apply Image Augmentation on Images and Labels using Albumentations\n",
    "This part sets up the Albumentation pipeline and tests it on 1 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b7ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ff78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=540, height=540), \n",
    "                         alb.HorizontalFlip(p=0.5), \n",
    "                         alb.RandomBrightnessContrast(p=0.2),\n",
    "                         alb.RandomGamma(p=0.2), \n",
    "                         alb.RGBShift(p=0.2), \n",
    "                         alb.VerticalFlip(p=0.5)], \n",
    "                       bbox_params=alb.BboxParams(format='albumentations', \n",
    "                                                  label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5249595",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join('data','train', 'images','8d8023dc-2eb0-11ee-b1eb-166359f0a21b.jpg'))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data', 'train', 'labels', '8d8023dc-2eb0-11ee-b1eb-166359f0a21b.json'), 'r') as f:\n",
    "    label = json.load(f)\n",
    "label['shapes'][0]['points']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db52920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the bounding box coordinates\n",
    "\n",
    "coords = [0,0,0,0]\n",
    "coords[0] = label['shapes'][0]['points'][0][0]\n",
    "coords[1] = label['shapes'][0]['points'][0][1]\n",
    "coords[2] = label['shapes'][0]['points'][1][0]\n",
    "coords[3] = label['shapes'][0]['points'][1][1]\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = list(np.divide(coords, [1280,720,1280,720]))\n",
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f71e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented['bboxes'][0][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1981a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'], \n",
    "              tuple(np.multiply(augmented['bboxes'][0][:2], [540,540]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:], [540,540]).astype(int)), \n",
    "                    (255,0,0), 2)\n",
    "\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e64c2ff",
   "metadata": {},
   "source": [
    "## Using the Albumentation Pipeline to Augment the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d49c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = os.path.abspath('')\n",
    "\n",
    "for partition in ['train','test','val']: \n",
    "    for image in os.listdir(os.path.join(current_folder, 'data', partition, 'images')):\n",
    "        img = cv2.imread(os.path.join(current_folder, 'data', partition, 'images', image))\n",
    "\n",
    "        coords = [0,0,0.00001,0.00001]\n",
    "        label_path = os.path.join(current_folder, 'data', partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "\n",
    "            coords[0] = label['shapes'][0]['points'][0][0]\n",
    "            coords[1] = label['shapes'][0]['points'][0][1]\n",
    "            coords[2] = label['shapes'][0]['points'][1][0]\n",
    "            coords[3] = label['shapes'][0]['points'][1][1]\n",
    "            coords = list(np.divide(coords, [1280,720,1280,720]))\n",
    "\n",
    "        try: \n",
    "            for x in range(50):\n",
    "                augmented = augmentor(image=img, bboxes=[coords], class_labels=['face'])\n",
    "                cv2.imwrite(os.path.join(current_folder, 'aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "\n",
    "                annotation = {}\n",
    "                annotation['image'] = image\n",
    "\n",
    "                if os.path.exists(label_path):\n",
    "                    if len(augmented['bboxes']) == 0: \n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0 \n",
    "                    else: \n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                else: \n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 \n",
    "\n",
    "\n",
    "                with open(os.path.join(current_folder,  'aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3448db4b",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- `tf.data.Dataset.list_files('aug_data/train/images/*.jpg', shuffle=False)` loads all the file names in the directory into a dataset.\n",
    "- `train_images.map(load_image)` applies the `load_image` function to each element of the dataset.\n",
    "- `train_images.map(lambda x: tf.image.resize(x, (120,120)))` applies a lambda function where  `tf.image.resize()` is called on each image and resizes them to 120x120.\n",
    "- `train_images.map(lambda x: x/255)` applies a lambda function where each image is divided by 255 to decrease the data size (to improve processing speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba4c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data/train/images/*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data/test/images/*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeef0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data/val/images/*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.as_numpy_iterator().next()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3451ed6",
   "metadata": {},
   "source": [
    "## Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(), 'r', encoding = \"utf-8\") as f:\n",
    "        label = json.load(f)\n",
    "        \n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8da8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data/train/labels/*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data/test/labels/*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0df452",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data/val/labels/*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea26bc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images), len(train_labels), len(test_images), len(test_labels), len(val_images), len(val_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bb12972",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- `zip()` function returns a zip object, which is an iterator of tuples where the first item in each passed iterator is paired together, and then the second item in each passed iterator are paired together etc.\n",
    "- `shuffle()` function takes a sequence (list, string, or tuple) and reorganize the order of the items.\n",
    "- `batch()` function combines consecutive elements of a dataset into batches.\n",
    "- `prefetch()` function allows later elements to be prepared while the current element is being processed. It can be used to overlap the preprocessing and model execution of a training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_images, train_labels))\n",
    "train = train.shuffle(5000)\n",
    "train = train.batch(8)\n",
    "train = train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6cbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tf.data.Dataset.zip((test_images, test_labels))\n",
    "test = test.shuffle(1300)\n",
    "test = test.batch(8)\n",
    "test = test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = tf.data.Dataset.zip((val_images, val_labels))\n",
    "val = val.shuffle(1000)\n",
    "val = val.batch(8)\n",
    "val = val.prefetch(4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d329825",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c57098",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e481ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = res[0][idx]\n",
    "    sample_coords = res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(sample_image, \n",
    "                  tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                        (255,0,0), 2)\n",
    "\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c0e15d4",
   "metadata": {},
   "source": [
    "## Build Deep Learning using the Functional API\n",
    "\n",
    "Notes:\n",
    "- Our problem is both a classification problem as we are trying to determine whether the object is a 'face', and it is also a regression problem because we are trying to predict the coordinates of the bounding box.\n",
    "- Keras is a higher level API that runs on top of TensorFlow. It is easier to use and has a simpler syntax while still allowing for full customization.\n",
    "- VGG16 is a convolutional neural network model that is pre-trained. We will cut off the last few layers of the model as it is meant to do classification but we need BOTH classification and regression (for localization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ea70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c323ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    input_layer = Input(shape=(120,120,3))\n",
    "    \n",
    "    vgg = VGG16(include_top=False)(input_layer)\n",
    "\n",
    "    # Classification Model  \n",
    "    f1 = GlobalMaxPooling2D()(vgg)\n",
    "    class1 = Dense(2048, activation='relu')(f1)\n",
    "    class2 = Dense(1, activation='sigmoid')(class1)\n",
    "    \n",
    "    # Bounding box model\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regress1 = Dense(2048, activation='relu')(f2)\n",
    "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
    "    \n",
    "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a24145",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684bb16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec79b2eb",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec8763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords = facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7569d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, coords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fcb4bd45",
   "metadata": {},
   "source": [
    "## Define Losses and Optimizers\n",
    "\n",
    "We're using Adam as our optimizer\n",
    "- Optimizer figure out how to apply the best gradients and effectively apply back-propagation across the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b287a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### this method is deprecated\n",
    "batches_per_epoch = len(train)\n",
    "# lr_decay = (1./0.75 -1)/batches_per_epoch\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)\n",
    "\n",
    "\n",
    "# this is to set up the learning rate to be 75% of the previous epoch's learning rate so \n",
    "# that we slow down the learning rate as we continue to train the model so that we don't \n",
    "# overfit and blow out the gradients\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.0001,\n",
    "    decay_steps=batches_per_epoch,\n",
    "    decay_rate=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fdadc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c4c5e3d",
   "metadata": {},
   "source": [
    "#### Create Localization Loss and Classification Loss\n",
    "\n",
    "Notes:\n",
    "- calculating binary cross entropy loss (aka logarithmic loss) for CLASSIFICATION LOSS\n",
    "- calculating localization loss for REGRESSION LOSS\n",
    "  1. we're getting the distance between our actual coordinate vs the predicted coordinate\n",
    "  2. we're getting the square of the height diff and the square of the width diff\n",
    "      - then we're getting the actual height & width of the bounding box\n",
    "      - then we're getting the predicted height & width of the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da83924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localization_loss(y_true, yhat):            \n",
    "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
    "                  \n",
    "    h_true = y_true[:,3] - y_true[:,1] \n",
    "    w_true = y_true[:,2] - y_true[:,0] \n",
    "\n",
    "    h_pred = yhat[:,3] - yhat[:,1] \n",
    "    w_pred = yhat[:,2] - yhat[:,0] \n",
    "    \n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
    "    \n",
    "    return delta_coord + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss = tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss = localization_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8700033f",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7fa066",
   "metadata": {},
   "outputs": [],
   "source": [
    "localization_loss(y[1], coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0], classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressloss(y[1], coords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c845a7d6",
   "metadata": {},
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb3273",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model): \n",
    "    def __init__(self, facetracker,  **kwargs): \n",
    "        super().__init__(**kwargs)\n",
    "        self.model = facetracker\n",
    "\n",
    "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.opt = opt\n",
    "        self.closs = classloss\n",
    "        self.lloss = localizationloss\n",
    "    \n",
    "    def train_step(self, batch, **kwargs): \n",
    "        \n",
    "        X, y = batch\n",
    "        \n",
    "        with tf.GradientTape() as tape: \n",
    "            classes, coords = self.model(X, training=True)\n",
    "            \n",
    "            batch_classloss = self.closs(y[0], classes)\n",
    "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "            \n",
    "            # this 0.5 is a hyperparameter that we can tune to see how much we want to penalize the classification loss\n",
    "            total_loss = batch_localizationloss + 0.5 * batch_classloss\n",
    "            \n",
    "            # calculates the gradients with respect to the loss function\n",
    "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "        \n",
    "        # applying one step of gradient descent for the backpropagation\n",
    "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self, batch, **kwargs): \n",
    "        X, y = batch\n",
    "        \n",
    "        classes, coords = self.model(X, training=False)\n",
    "        \n",
    "        batch_classloss = self.closs(y[0], classes)\n",
    "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
    "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        # notice no backpropagation here !!\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
    "        \n",
    "    def call(self, X, **kwargs): \n",
    "        return self.model(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95060a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt, classloss, regressloss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33659fc6",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89160ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e1141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train, epochs=10, validation_data=val, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd8ec76",
   "metadata": {},
   "source": [
    "## Plotting Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c3a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bcfdb4a8",
   "metadata": {},
   "source": [
    "## Making Predictions / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5751f4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e953d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc4ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2790cbe5",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbe0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77565b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker = load_model('facetracker.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06728b",
   "metadata": {},
   "source": [
    "## Real-Time Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0885c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "    _ , frame = cap.read()\n",
    "    frame = frame[50:500, 50:500,:]\n",
    "    \n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    resized = tf.image.resize(rgb, (120,120))\n",
    "    \n",
    "    yhat = facetracker.predict(np.expand_dims(resized/255,0))\n",
    "    sample_coords = yhat[1][0]\n",
    "    \n",
    "    if yhat[0] > 0.9: \n",
    "        # Controls the main rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.multiply(sample_coords[:2], [540,540]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [540,540]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "        # Controls the label rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [540,540]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(sample_coords[:2], [540,540]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        # Controls the text rendered\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(sample_coords[:2], [540,540]).astype(int), [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    cv2.imshow('EyeTrack', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
